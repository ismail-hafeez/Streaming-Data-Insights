{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54093it [00:05, 9812.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sample_json(input_file, output_file, target_size_gb, filter_key='also_buy'):\n",
    "    target_size_bytes = target_size_gb * 1024**3\n",
    "    current_size_bytes = 0\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in tqdm(infile):\n",
    "            record = json.loads(line)\n",
    "            if record.get(filter_key):\n",
    "                outfile.write(json.dumps(record) + '\\n')\n",
    "                current_size_bytes += len(line.encode('utf-8'))\n",
    "\n",
    "            if current_size_bytes >= target_size_bytes:\n",
    "                break\n",
    "\n",
    "    print(\"Finished\")\n",
    "\n",
    "sample_json('Sampled_Meta_Data.json', 'small_sample.json', 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Open the JSON file\n",
    "with open('Sampled_Meta_Data.json') as f:\n",
    "    # Initialize an empty list to store the JSON objects\n",
    "    data_list = []\n",
    "\n",
    "    # Iterate through each line in the file\n",
    "    for line in f:\n",
    "        try:\n",
    "            # Attempt to load each line as a JSON object and append it to the list\n",
    "            data = json.loads(line)\n",
    "            data_list.append(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Initialize an empty dictionary to store values\n",
    "result_dict = {}\n",
    "\n",
    "# Iterate through the list of dictionaries\n",
    "for data in data_list:\n",
    "    # Iterate through key-value pairs in each dictionary\n",
    "    for key, value in data.items():\n",
    "        # If the key is not in result_dict, create a list with the current value\n",
    "        if key not in result_dict:\n",
    "            result_dict[key] = [value]\n",
    "        else:\n",
    "            # If the key is already in result_dict, append the current value to the list\n",
    "            result_dict[key].append(value)\n",
    "\n",
    "# creating dataframe\n",
    "data_frame = pd.DataFrame(result_dict)\n",
    "\n",
    "# dropping rows with any missing values\n",
    "data_frame = data_frame.dropna(how='any')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_large_file(input_file, batch_size=100000):\n",
    "    # Open the JSON file\n",
    "    with open(input_file) as f:\n",
    "        batch_count = 0\n",
    "        total_records = 0\n",
    "        batch_data = []\n",
    "\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Attempt to load each line as a JSON object\n",
    "                data = json.loads(line)\n",
    "                batch_data.append(data)\n",
    "                total_records += 1\n",
    "\n",
    "                # Check if batch size is reached\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    # Process the current batch\n",
    "                    process_batch(batch_data, batch_count)\n",
    "                    batch_count += 1\n",
    "                    batch_data = []\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "        # Process the remaining records in the last batch\n",
    "        if batch_data:\n",
    "            process_batch(batch_data, batch_count)\n",
    "\n",
    "        print(f\"Total records processed: {total_records}\")\n",
    "\n",
    "# Function to process each batch\n",
    "def process_batch(batch_data, batch_count):\n",
    "    # Convert batch data to DataFrame\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    # Perform data processing operations\n",
    "\n",
    "    # dropping rows with any missing values\n",
    "    batch_df = batch_df.dropna(how='any')\n",
    "\n",
    "    # removing unnecessary column headers\n",
    "    processed_batch = data_frame.drop(['title','also_view','price','feature','description', 'brand','category','date', 'fit', 'tech1', 'tech2', 'image', 'details', 'main_cat', 'similar_item', 'rank'], axis=1)\n",
    "\n",
    "    processed_batch['asin'] = processed_batch['asin'].astype('object')\n",
    "    processed_batch['also_buy'] = processed_batch['also_buy'].astype('object')   \n",
    "\n",
    "    # Save processed batch to a new file or perform further operations\n",
    "    output_file = f\"output_batch_{batch_count}.json\"\n",
    "    processed_batch.to_json(output_file, orient='records')\n",
    "\n",
    "    print(f\"Batch {batch_count} processed and saved to {output_file}\")\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'Sampled_Meta_Data.json'\n",
    "    process_large_file(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also_buy</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B077GQQKRV, B07CBJQTF6, B07H2Z6S9J, B06Y26PZ5...</td>\n",
       "      <td>6342509379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B018YRBB80, B07FD9HWPM, B017M55DI4, B07KX6PPW...</td>\n",
       "      <td>6342502315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[B00VBVXVPI]</td>\n",
       "      <td>6342522545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[B01AHZSZ9A, B01I809NCO, B07219C7LQ, B06ZZBQMT...</td>\n",
       "      <td>6342522898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[B06XY5N95G, B01LY4VKTL, B01EKRMG8C, B004SLKRY...</td>\n",
       "      <td>6342523002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            also_buy        asin\n",
       "0  [B077GQQKRV, B07CBJQTF6, B07H2Z6S9J, B06Y26PZ5...  6342509379\n",
       "1  [B018YRBB80, B07FD9HWPM, B017M55DI4, B07KX6PPW...  6342502315\n",
       "2                                       [B00VBVXVPI]  6342522545\n",
       "3  [B01AHZSZ9A, B01I809NCO, B07219C7LQ, B06ZZBQMT...  6342522898\n",
       "4  [B06XY5N95G, B01LY4VKTL, B01EKRMG8C, B004SLKRY...  6342523002"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe\n",
    "data_frame = pd.DataFrame(result_dict)\n",
    "\n",
    "# dropping rows with any missing values\n",
    "data_frame = data_frame.dropna(how='any')\n",
    "\n",
    "# Apply the function to the 'price' column\n",
    "#df['price'] = df['price'].apply(extract_first_price)\n",
    "\n",
    "# removing unnecessary column headers\n",
    "df = data_frame.drop(['title','also_view','price','feature','description', 'brand','category','date', 'fit', 'tech1', 'tech2', 'image', 'details', 'main_cat', 'similar_item', 'rank'], axis=1)\n",
    "\n",
    "# Function to extract first value from price strings\n",
    "def extract_first_price(price_str):\n",
    "    # Split the string by '-'\n",
    "    price_parts = price_str.split(' - ')\n",
    "    # Extract the first part\n",
    "    first_price = price_parts[0]\n",
    "    return first_price\n",
    "\n",
    "df['asin'] = df['asin'].astype('object')\n",
    "df['also_buy'] = df['also_buy'].astype('object')\n",
    "\n",
    "# Print the modified DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crazy Women's Voile Crinkle Scarf Shawl\n"
     ]
    }
   ],
   "source": [
    "value = df.iloc[1, 0]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dateframe successfully written in outputFile.json\n"
     ]
    }
   ],
   "source": [
    "# saving as json file\n",
    "output_file = 'outputFile.json'\n",
    "df.to_json(output_file, orient='records')\n",
    "\n",
    "print(f\"Dateframe successfully written in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6342502315'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge columns into a list\n",
    "merged_list = df.apply(lambda row: [row['asin'], row['also_buy']], axis=1)\n",
    "\n",
    "# Flatten the nested list\n",
    "flattened_list = [item for sublist in merged_list for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "flattened_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also_buy</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[B077GQQKRV, B07CBJQTF6, B07H2Z6S9J, B06Y26PZ5...</td>\n",
       "      <td>6342509379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[B018YRBB80, B07FD9HWPM, B017M55DI4, B07KX6PPW...</td>\n",
       "      <td>6342502315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[B00VBVXVPI]</td>\n",
       "      <td>6342522545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[B01AHZSZ9A, B01I809NCO, B07219C7LQ, B06ZZBQMT...</td>\n",
       "      <td>6342522898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[B06XY5N95G, B01LY4VKTL, B01EKRMG8C, B004SLKRY...</td>\n",
       "      <td>6342523002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            also_buy        asin\n",
       "0  [B077GQQKRV, B07CBJQTF6, B07H2Z6S9J, B06Y26PZ5...  6342509379\n",
       "1  [B018YRBB80, B07FD9HWPM, B017M55DI4, B07KX6PPW...  6342502315\n",
       "2                                       [B00VBVXVPI]  6342522545\n",
       "3  [B01AHZSZ9A, B01I809NCO, B07219C7LQ, B06ZZBQMT...  6342522898\n",
       "4  [B06XY5N95G, B01LY4VKTL, B01EKRMG8C, B004SLKRY...  6342523002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = data_frame.drop(['title','also_view','price','feature','description', 'brand','category','date', 'fit', 'tech1', 'tech2', 'image', 'details', 'main_cat', 'similar_item', 'rank'], axis=1)\n",
    "\n",
    "df_new.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B000000XB8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B001RNO30W)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B001SES07C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B002YAGIOG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B00318CBS2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B003AIKE3C)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B003AIKE6E)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B004GGU9QY)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B005TS13Y0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B007KFQ3E0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B007P28FN0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B009A882D0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B00KX22PBC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B00P2D5U7I)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B00W9DYAIY)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B00W9DYBDS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B012S37HQ6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B013O1T4NA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B01D10RT22)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B01HNOIS4A)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B01LF76SJS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B01LX9T2O1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B01M3R75G6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B07BRDZVK2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B07BT98MVN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B07C5F6YDF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.012407</td>\n",
       "      <td>(B07CM33RK6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.014888</td>\n",
       "      <td>(B07D1TRJ94)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     support      itemsets\n",
       "0   0.012407  (B000000XB8)\n",
       "1   0.012407  (B001RNO30W)\n",
       "2   0.012407  (B001SES07C)\n",
       "3   0.012407  (B002YAGIOG)\n",
       "4   0.012407  (B00318CBS2)\n",
       "5   0.012407  (B003AIKE3C)\n",
       "6   0.012407  (B003AIKE6E)\n",
       "7   0.012407  (B004GGU9QY)\n",
       "8   0.012407  (B005TS13Y0)\n",
       "9   0.012407  (B007KFQ3E0)\n",
       "10  0.012407  (B007P28FN0)\n",
       "11  0.012407  (B009A882D0)\n",
       "12  0.012407  (B00KX22PBC)\n",
       "13  0.012407  (B00P2D5U7I)\n",
       "14  0.012407  (B00W9DYAIY)\n",
       "15  0.012407  (B00W9DYBDS)\n",
       "16  0.012407  (B012S37HQ6)\n",
       "17  0.012407  (B013O1T4NA)\n",
       "18  0.012407  (B01D10RT22)\n",
       "19  0.012407  (B01HNOIS4A)\n",
       "20  0.012407  (B01LF76SJS)\n",
       "21  0.012407  (B01LX9T2O1)\n",
       "22  0.012407  (B01M3R75G6)\n",
       "23  0.012407  (B07BRDZVK2)\n",
       "24  0.012407  (B07BT98MVN)\n",
       "25  0.012407  (B07C5F6YDF)\n",
       "26  0.012407  (B07CM33RK6)\n",
       "27  0.014888  (B07D1TRJ94)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "\n",
    "df_new['asin'] = df_new['asin'].astype('object')\n",
    "df_new['also_buy'] = df_new['also_buy'].astype('object')\n",
    "\n",
    "#data = df_new.iloc[:20]\n",
    "\n",
    "# Flatten the 'also_buy' lists and handle non-iterable elements\n",
    "transactions = []\n",
    "for sublist in data['also_buy']:\n",
    "    if isinstance(sublist, list):\n",
    "        transactions.extend(sublist)\n",
    "    else:\n",
    "        transactions.append(sublist)\n",
    "\n",
    "# Convert the transactions into a list of lists format\n",
    "transactions = [[str(item) for item in sublist] if isinstance(sublist, list) else [str(sublist)] for sublist in transactions]\n",
    "\n",
    "# Encode the transactions into a binary format suitable for Apriori\n",
    "encoder = TransactionEncoder()\n",
    "transactions_encoded = encoder.fit_transform(transactions)\n",
    "\n",
    "# Convert the encoded transactions into a DataFrame\n",
    "df1 = pd.DataFrame(transactions_encoded, columns=encoder.columns_)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df1, min_support=0.01, use_colnames=True)\n",
    "\n",
    "frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 processed and saved to output_batch_0.json\n",
      "Batch 1 processed and saved to output_batch_1.json\n",
      "Batch 2 processed and saved to output_batch_2.json\n",
      "Batch 3 processed and saved to output_batch_3.json\n",
      "Batch 4 processed and saved to output_batch_4.json\n",
      "Batch 5 processed and saved to output_batch_5.json\n",
      "Batch 6 processed and saved to output_batch_6.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['details'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     60\u001b[0m     input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSampled_Meta_Data.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mprocess_large_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m, in \u001b[0;36mprocess_large_file\u001b[0;34m(input_file, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Check if batch size is reached\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch_data) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Process the current batch\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     25\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 47\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(batch_data, batch_count)\u001b[0m\n\u001b[1;32m     44\u001b[0m batch_df \u001b[38;5;241m=\u001b[39m batch_df\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# removing unnecessary column headers\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m processed_batch \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malso_view\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtech1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtech2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetails\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmain_cat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimilar_item\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m processed_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m processed_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124masin\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m processed_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malso_buy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m processed_batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malso_buy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m'\u001b[39m)   \n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/frame.py:5399\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   5252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5260\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5263\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5264\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5397\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5405\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5406\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/generic.py:4505\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4505\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/generic.py:4546\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4544\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4546\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4547\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4549\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4550\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:6934\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6934\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6935\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['details'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_large_file(input_file, batch_size=100000):\n",
    "    # Open the JSON file\n",
    "    with open(input_file) as f:\n",
    "        batch_count = 0\n",
    "        total_records = 0\n",
    "        batch_data = []\n",
    "\n",
    "        # Iterate through each line in the file\n",
    "        for line in f:\n",
    "            try:\n",
    "                # Attempt to load each line as a JSON object\n",
    "                data = json.loads(line)\n",
    "                batch_data.append(data)\n",
    "                total_records += 1\n",
    "\n",
    "                # Check if batch size is reached\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    # Process the current batch\n",
    "                    process_batch(batch_data, batch_count)\n",
    "                    batch_count += 1\n",
    "                    batch_data = []\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "        # Process the remaining records in the last batch\n",
    "        if batch_data:\n",
    "            process_batch(batch_data, batch_count)\n",
    "\n",
    "        print(f\"Total records processed: {total_records}\")\n",
    "\n",
    "# Function to process each batch\n",
    "def process_batch(batch_data, batch_count):\n",
    "    # Convert batch data to DataFrame\n",
    "    batch_df = pd.DataFrame(batch_data)\n",
    "\n",
    "    # Perform data processing operations\n",
    "\n",
    "    # dropping rows with any missing values\n",
    "    batch_df = batch_df.dropna(how='any')\n",
    "\n",
    "    # removing unnecessary column headers\n",
    "    processed_batch = batch_df.drop(['title','also_view','price','feature','description', 'brand','category','date', 'fit', 'tech1', 'tech2', 'image', 'details', 'main_cat', 'similar_item', 'rank'], axis=1)\n",
    "\n",
    "    processed_batch['asin'] = processed_batch['asin'].astype('object')\n",
    "    processed_batch['also_buy'] = processed_batch['also_buy'].astype('object')   \n",
    "\n",
    "    # Save processed batch to a new file or perform further operations\n",
    "    output_file = f\"output_batch_{batch_count}.json\"\n",
    "    processed_batch.to_json(output_file, orient='records')\n",
    "\n",
    "    print(f\"Batch {batch_count} processed and saved to {output_file}\")\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'Sampled_Meta_Data.json'\n",
    "    process_large_file(input_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
